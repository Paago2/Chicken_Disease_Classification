{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\paago\\\\Documents\\\\Chicken_Disease_Classification\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\paago\\\\Documents\\\\Chicken_Disease_Classification'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source: str\n",
    "    source_csv: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n",
    "    train_size: float = None\n",
    "    test_size: float = None\n",
    "    validation_size: float = None\n",
    "    random_state: int = None\n",
    "    max_samples: int = None\n",
    "    min_samples: int = None\n",
    "    img_size: list = None\n",
    "    working_dir: str = None\n",
    "    batch_size: int = None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import yaml\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from cnnClassifier.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories, get_size\n",
    "from cnnClassifier.entity.config_entity import DataIngestionConfig\n",
    "from cnnClassifier import logger\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        print(\"Config: \", self.config)\n",
    "        self.params = read_yaml(params_filepath)  \n",
    "        print(\"Params: \", self.params)\n",
    "        create_directories([self.config.data_ingestion.root_dir])\n",
    "\n",
    "    def copy_zip_data(self):\n",
    "        source_path = self.config.data_ingestion.source\n",
    "        destination_path = self.config.data_ingestion.local_data_file\n",
    "        if not os.path.exists(destination_path):\n",
    "            shutil.copy2(source_path, destination_path)\n",
    "            logger.info(f\"Zip file copied from {source_path} to {destination_path}.\")\n",
    "        else:\n",
    "            logger.info(f\"File {destination_path} already exists.\")\n",
    "\n",
    "    def unzip_data(self):\n",
    "        try:\n",
    "            with zipfile.ZipFile(self.config.data_ingestion.local_data_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.config.data_ingestion.unzip_dir)\n",
    "            logger.info(\"Zip file extracted successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Zip file not found at path: {self.config.data_ingestion.local_data_file}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while extracting zip file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "        params = self.params.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir, params.working_dir])\n",
    "        self.copy_zip_data()\n",
    "        self.unzip_data()\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source=config.source,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir,\n",
    "            source_csv=config.source_csv,\n",
    "            train_size=params.train_size,\n",
    "            test_size=params.test_size,\n",
    "            validation_size=params.validation_size,\n",
    "            random_state=params.random_state,\n",
    "            max_samples=params.max_samples,\n",
    "            min_samples=params.min_samples,\n",
    "            img_size=params.img_size,\n",
    "            working_dir=params.working_dir\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    \n",
    "    def load_csv(self):\n",
    "        logger.info(\"Loading CSV file.\")\n",
    "        df = pd.read_csv(self.config.source_csv)\n",
    "        df.columns=['filepaths', 'labels']\n",
    "        df['filepaths'] = df['filepaths'].apply(lambda x: os.path.join(self.config.root_dir, x))\n",
    "        logger.info(f\"CSV file loaded successfully with {df.shape[0]} rows.\")\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def split_data(self, df):\n",
    "        logger.info(\"Splitting data into training, testing, and validation sets.\")\n",
    "        trsplit = self.config.train_size\n",
    "        vsplit = self.config.validation_size\n",
    "        dsplit = vsplit / (1 - trsplit)\n",
    "        strat = df['labels']\n",
    "        train_df, dummy_df = train_test_split(df, train_size=trsplit, shuffle=True, random_state=self.config.random_state, stratify=strat)\n",
    "        strat = dummy_df['labels']\n",
    "        test_df, valid_df = train_test_split(dummy_df, train_size=dsplit, shuffle=True, random_state=self.config.random_state, stratify=strat)\n",
    "        logger.info(\"Data split successfully.\")\n",
    "        logger.info(f\"Train set size: {train_df.shape[0]}\")\n",
    "        logger.info(f\"Test set size: {test_df.shape[0]}\")\n",
    "        logger.info(f\"Validation set size: {valid_df.shape[0]}\")\n",
    "\n",
    "        return train_df, test_df, valid_df\n",
    "\n",
    "\n",
    "\n",
    "    def class_distribution(self, train_df):\n",
    "        logger.info(\"Getting class distribution.\")\n",
    "        groups = train_df.groupby('labels')  \n",
    "        print('{0:^30s} {1:^13s}'.format('CLASS', 'IMAGE COUNT'))\n",
    "        for label in train_df['labels'].unique():\n",
    "            print('{0:^30s} {1:^13d}'.format(label, len(groups.get_group(label))))\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "    def trim(self, train_df):\n",
    "        logger.info(\"Trimming classes.\")\n",
    "        max_size = self.config.max_samples\n",
    "        min_size = self.config.min_samples\n",
    "        column = 'labels'\n",
    "        train_df = train_df.copy()\n",
    "        original_class_count= len(list(train_df[column].unique()))\n",
    "        logger.info('Original Number of classes in dataframe: %s', original_class_count)\n",
    "        sample_list=[] \n",
    "        groups=train_df.groupby(column)\n",
    "        for label in train_df[column].unique():        \n",
    "            group=groups.get_group(label)\n",
    "            sample_count=len(group)         \n",
    "            if sample_count> max_size :\n",
    "                strat=group[column]\n",
    "                samples,_=train_test_split(group, train_size=max_size, shuffle=True, random_state=self.config.random_state, stratify=strat)            \n",
    "                sample_list.append(samples)\n",
    "            elif sample_count>= min_size:\n",
    "                sample_list.append(group)\n",
    "        train_df=pd.concat(sample_list, axis=0).reset_index(drop=True)\n",
    "        final_class_count= len(list(train_df[column].unique())) \n",
    "        if final_class_count != original_class_count:\n",
    "            logger.warning('*** WARNING***  dataframe has a reduced number of classes' )\n",
    "        balance=list(train_df[column].value_counts())\n",
    "        logger.info('Class balance: %s', balance)\n",
    "        return train_df\n",
    "    \n",
    "    def prepare_working_dir(self):\n",
    "        logger.info(\"Preparing working directory.\")\n",
    "        os.makedirs(self.config.working_dir, exist_ok=True)\n",
    "        logger.info(\"Working directory prepared.\")\n",
    "\n",
    "    \n",
    "    def execute(self):\n",
    "        logger.info(\"Starting data ingestion.\")\n",
    "        df = self.load_csv()\n",
    "        train_df, test_df, valid_df = self.split_data(df)\n",
    "        self.class_distribution(train_df)\n",
    "        train_df = self.trim(train_df)\n",
    "        self.prepare_working_dir()\n",
    "\n",
    "        # save the dataframes to CSV files\n",
    "        print(\"About to write train.csv\")\n",
    "        train_df.to_csv(os.path.join(self.config.working_dir, 'train.csv'), index=False)\n",
    "        print(\"Finished writing train.csv\")\n",
    "\n",
    "        test_df.to_csv(os.path.join(self.config.working_dir, 'test.csv'), index=False)\n",
    "        valid_df.to_csv(os.path.join(self.config.working_dir, 'valid.csv'), index=False)\n",
    "\n",
    "\n",
    "        logger.info(\"Data ingestion completed successfully.\")\n",
    "        return train_df, test_df, valid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 04:48:10,636: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "Config:  {'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source': 'C:\\\\\\\\Users\\\\\\\\paago\\\\\\\\Downloads\\\\\\\\Chicken_Fecal.zip', 'local_data_file': 'artifacts/data_ingestion/data.zip', 'unzip_dir': 'artifacts/data_ingestion', 'source_csv': 'artifacts\\\\\\\\data_ingestion\\\\\\\\Fecal_data.csv'}}\n",
      "[2023-07-31 04:48:10,641: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "Params:  {'base_model': {'image_size': [224, 224], 'model_name': 'EfficientNetB5', 'weights': 'imagenet', 'include_top': False, 'pooling': 'max'}, 'full_model': {'dropout_rate1': 0.3, 'dropout_rate2': 0.45, 'dense_1024_regularizer_l2': 0.016, 'dense_1024_regularizer_l1': 0.006, 'dense_128_regularizer_l2': 0.016, 'dense_128_regularizer_l1': 0.006}, 'learning_rate': 0.001, 'classes': 10, 'epochs': 50, 'ask_epoch': 10, 'data_ingestion': {'train_size': 0.9, 'test_size': 0.05, 'validation_size': 0.05, 'random_state': 123, 'max_samples': 500, 'min_samples': 0, 'img_size': [224, 224], 'working_dir': './working_dir'}}\n",
      "[2023-07-31 04:48:10,643: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2023-07-31 04:48:10,644: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2023-07-31 04:48:10,646: INFO: common: created directory at: ./working_dir]\n",
      "[2023-07-31 04:48:10,787: INFO: 4074483130: Zip file copied from C:\\\\Users\\\\paago\\\\Downloads\\\\Chicken_Fecal.zip to artifacts/data_ingestion/data.zip.]\n",
      "[2023-07-31 04:48:17,815: INFO: 4074483130: Zip file extracted successfully.]\n"
     ]
    }
   ],
   "source": [
    "# Create a ConfigurationManager instance and fetch data ingestion config\n",
    "cm = ConfigurationManager()\n",
    "config = cm.get_data_ingestion_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 04:48:35,646: INFO: 3530614321: Starting data ingestion.]\n",
      "[2023-07-31 04:48:35,647: INFO: 3530614321: Loading CSV file.]\n",
      "[2023-07-31 04:48:35,683: INFO: 3530614321: CSV file loaded successfully with 8067 rows.]\n",
      "[2023-07-31 04:48:35,683: INFO: 3530614321: Splitting data into training, testing, and validation sets.]\n",
      "[2023-07-31 04:48:35,692: INFO: 3530614321: Data split successfully.]\n",
      "[2023-07-31 04:48:35,693: INFO: 3530614321: Train set size: 7260]\n",
      "[2023-07-31 04:48:35,693: INFO: 3530614321: Test set size: 403]\n",
      "[2023-07-31 04:48:35,694: INFO: 3530614321: Validation set size: 404]\n",
      "[2023-07-31 04:48:35,695: INFO: 3530614321: Getting class distribution.]\n",
      "            CLASS               IMAGE COUNT \n",
      "         Coccidiosis               2228     \n",
      "          Salmonella               2362     \n",
      "           Healthy                 2164     \n",
      "      New Castle Disease            506     \n",
      "\n",
      "\n",
      "[2023-07-31 04:48:35,699: INFO: 3530614321: Trimming classes.]\n",
      "[2023-07-31 04:48:35,702: INFO: 3530614321: Original Number of classes in dataframe: 4]\n",
      "[2023-07-31 04:48:35,714: INFO: 3530614321: Class balance: [500, 500, 500, 500]]\n",
      "[2023-07-31 04:48:35,715: INFO: 3530614321: Preparing working directory.]\n",
      "[2023-07-31 04:48:35,716: INFO: 3530614321: Working directory prepared.]\n",
      "About to write train.csv\n",
      "Finished writing train.csv\n",
      "[2023-07-31 04:48:35,730: INFO: 3530614321: Data ingestion completed successfully.]\n",
      "Train DataFrame:\n",
      "                                 filepaths       labels\n",
      "0  artifacts/data_ingestion\\cocci.1088.jpg  Coccidiosis\n",
      "1   artifacts/data_ingestion\\cocci.619.jpg  Coccidiosis\n",
      "2   artifacts/data_ingestion\\cocci.771.jpg  Coccidiosis\n",
      "3   artifacts/data_ingestion\\cocci.905.jpg  Coccidiosis\n",
      "4   artifacts/data_ingestion\\cocci.618.jpg  Coccidiosis\n",
      "\n",
      "Test DataFrame:\n",
      "                                      filepaths              labels\n",
      "7931  artifacts/data_ingestion\\healthy.1270.jpg             Healthy\n",
      "3010  artifacts/data_ingestion\\pcrsalmo.263.jpg          Salmonella\n",
      "3545       artifacts/data_ingestion\\ncd.133.jpg  New Castle Disease\n",
      "934   artifacts/data_ingestion\\pcrcocci.338.jpg         Coccidiosis\n",
      "6511   artifacts/data_ingestion\\pcrcocci.79.jpg         Coccidiosis\n",
      "\n",
      "Validation DataFrame:\n",
      "                                        filepaths              labels\n",
      "1116       artifacts/data_ingestion\\cocci.549.jpg         Coccidiosis\n",
      "3702  artifacts/data_ingestion\\pcrhealthy.315.jpg             Healthy\n",
      "2          artifacts/data_ingestion\\cocci.171.jpg         Coccidiosis\n",
      "4888      artifacts/data_ingestion\\salmo.1239.jpg          Salmonella\n",
      "6455         artifacts/data_ingestion\\ncd.211.jpg  New Castle Disease\n"
     ]
    }
   ],
   "source": [
    "data_ingestion = DataIngestion(config)\n",
    "train_df, test_df, valid_df = data_ingestion.execute()\n",
    "\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df.head())\n",
    "print(\"\\nValidation DataFrame:\")\n",
    "print(valid_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
